{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08895554",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pathlib\n",
    "import torch\n",
    "import esm\n",
    "from esm import FastaBatchedDataset, pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92bf5365",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downlaod esm2 model(ems2 model takes time while dowoloading, after that code takes few seconds to run) and embeddings etraction\n",
    "def extract_embeddings(output_dir, tokens_per_batch=4096, seq_length=7096,repr_layers=[36]):\n",
    "    \n",
    "    model, alphabet = esm.pretrained.esm2_t36_3B_UR50D()\n",
    "    model.eval()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        \n",
    "    dataset = FastaBatchedDataset.from_file(fasta_file)\n",
    "    batches = dataset.get_batch_indices(tokens_per_batch, extra_toks_per_seq=1)\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, \n",
    "        collate_fn=alphabet.get_batch_converter(seq_length), \n",
    "        batch_sampler=batches\n",
    "    )\n",
    "\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    filenames = []  \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (labels, strs, toks) in enumerate(data_loader):\n",
    "\n",
    "            print(f'Processing batch {batch_idx + 1} of {len(batches)}')\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                toks = toks.to(device=\"cuda\", non_blocking=True)\n",
    "\n",
    "            out = model(toks, repr_layers=repr_layers, return_contacts=False)\n",
    "\n",
    "            logits = out[\"logits\"].to(device=\"cpu\")\n",
    "            representations = {layer: t.to(device=\"cpu\") for layer, t in out[\"representations\"].items()}\n",
    "            \n",
    "            for i, label in enumerate(labels):\n",
    "                entry_id = label.split()[0]\n",
    "                filename = output_dir / f\"{entry_id}.pt\"\n",
    "                filenames.append(filename)  \n",
    "                truncate_len = min(seq_length, len(strs[i]))\n",
    "\n",
    "                result = {\"entry_id\": entry_id}\n",
    "                result[\"mean_representations\"] = {\n",
    "                        layer: t[i, 1 : truncate_len + 1].mean(0).clone()\n",
    "                        for layer, t in representations.items()\n",
    "                    }\n",
    "\n",
    "                torch.save(result, filename)\n",
    "    return filenames  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cd1b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy paste query sequence to query_sequence.txt file \n",
    "fasta_file = pathlib.Path('LLPS_Pred/query_sequences/query_sequence.txt')\n",
    "output_dir = pathlib.Path('LLPS_Pred/embeddings/')\n",
    "#run the function\n",
    "extract_embeddings(fasta_file, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "540d9439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: example1.pt, Prediction: LLPS\n",
      "File: example2.pt, Prediction: non-LLPS\n",
      "File: example3.pt, Prediction: non-LLPS\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=512, kernel_size=3)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(512)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=4)\n",
    "        self.conv2 = nn.Conv1d(in_channels=512, out_channels=256, kernel_size=3)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.fc_input_size = self.calculate_fc_input_size()\n",
    "        self.fc1 = nn.Linear(self.fc_input_size, 128)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 2560)\n",
    "\n",
    "        x = self.pool(F.relu(self.batchnorm1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.batchnorm2(self.conv2(x))))\n",
    "\n",
    "        self.fc_input_size = x.view(x.size(0), -1).size(1)\n",
    "        x = x.view(-1, self.fc_input_size)\n",
    "\n",
    "        x = F.relu(self.batchnorm3(self.fc1(x)))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def calculate_fc_input_size(self):\n",
    "        x = torch.randn(1, 1, 2560)\n",
    "        x = self.pool(F.relu(self.batchnorm1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.batchnorm2(self.conv2(x))))\n",
    "        return x.view(x.size(0), -1).size(1)\n",
    "\n",
    "# load embeddings\n",
    "folder_path = 'LLPS_Pred/embeddings/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "\n",
    "def load_protein_representations():\n",
    "    queryproteinrep = []\n",
    "    for file_name in files:\n",
    "        rep_changes = torch.load(os.path.join(folder_path, file_name))['mean_representations'][36]\n",
    "        queryproteinrep.append(rep_changes.tolist())\n",
    "    return torch.Tensor(queryproteinrep).unsqueeze(1)\n",
    "\n",
    "query_rep = load_protein_representations()\n",
    "\n",
    "# Define the custom dataset class\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "dataset = ProteinDataset(query_rep)\n",
    "\n",
    "batch_size = 512\n",
    "dataloader_val = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Load model\n",
    "load_model_path = 'LLPS_Pred/model/model.pth'\n",
    "loaded_model = torch.load(load_model_path)\n",
    "loaded_model.eval()\n",
    "\n",
    "predicted_labels = []\n",
    "with torch.no_grad():\n",
    "    for data_val, file_name in zip(dataloader_val, files):\n",
    "        output_val = loaded_model(data_val)\n",
    "        _, predicted = torch.max(output_val, 1)\n",
    "        predicted_labels.extend(predicted.cpu().numpy())  \n",
    "\n",
    "# pridicted results\n",
    "for file_name, label in zip(files, predicted_labels):\n",
    "    print(f\"File: {file_name}, Prediction: {'LLPS' if label == 0 else 'non-LLPS'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6aa671",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
